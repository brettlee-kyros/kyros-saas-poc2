# Story 0.2: Validate Test Data Compatibility

## Status
Ready for Review

## Story
**As a** developer,
**I want** to validate test data format and structure from sample repos,
**So that** mock data preparation in Epic 4 has verified compatible source data.

## Acceptance Criteria
1. Verify `burn-performance-test-data/` directory exists with data files
2. Verify `mixshift-test-data/` directory exists with data files
3. Inspect data file formats (CSV, Parquet, JSON, etc.) and document in `docs/external-dependencies.md`
4. Load sample data files into Pandas DataFrames and verify successful parsing
5. Document data schemas (column names, types, sample row counts) for each dataset
6. Identify columns that will need tenant_id augmentation for multi-tenant filtering
7. Verify data files contain sufficient records for meaningful visualization (>20 rows recommended)
8. If test data missing or incompatible, create synthetic test data with documented schema
9. Document data preparation steps required for Epic 4, Story 4.1

## Tasks / Subtasks
- [x] **Task 1: Locate and verify test data directories** (AC: 1, 2)
  - [x] Check if `burn-performance-test-data/` directory exists
  - [x] Check if `mixshift-test-data/` directory exists
  - [x] List all data files in each directory
  - [x] Document file names, extensions, and sizes
  - [x] Verify read permissions on data files

- [x] **Task 2: Inspect data file formats and load into Pandas** (AC: 3, 4)
  - [x] Identify file formats (CSV, Parquet, JSON, Excel, etc.)
  - [x] Write Python script to load each file into Pandas DataFrame
  - [x] Handle different file formats (pd.read_csv, pd.read_parquet, pd.read_json)
  - [x] Document any parsing errors or warnings
  - [x] Save successful loading scripts for future reference

- [x] **Task 3: Document data schemas** (AC: 5, 7)
  - [x] For each dataset, extract and document:
    - Column names
    - Data types (int, float, string, datetime)
    - Sample values (first 3 rows)
    - Row count
    - Null value counts per column
  - [x] Verify row counts meet minimum threshold (>20 rows)
  - [x] Document schema in `docs/external-dependencies.md` under "Test Data Schemas" section

- [x] **Task 4: Identify tenant_id augmentation requirements** (AC: 6)
  - [x] Review existing columns in each dataset
  - [x] Determine if tenant_id column already exists
  - [x] If not, document where tenant_id should be added (column position, data type)
  - [x] Document augmentation strategy:
    - How to split data across tenants (50/50, 60/40, etc.)
    - Sample tenant_id values (UUIDs from seed data)
    - Column insertion method (pandas insert or assign)

- [x] **Task 5: Document Epic 4 data preparation steps** (AC: 9)
  - [x] Add "Epic 4 Data Preparation Steps" section to `docs/external-dependencies.md`
  - [x] Document step-by-step process:
    1. Copy test data files to `data/mock-data/`
    2. Add tenant_id column to each dataset
    3. Distribute records across tenants
    4. Save augmented data files
  - [x] Provide code snippets for tenant_id augmentation
  - [x] Note any data transformations needed (date parsing, type conversions)

- [x] **Task 6: Create synthetic data if needed** (AC: 8)
  - [x] If test data missing: Create synthetic data generation script
  - [x] Match expected schema for CLV data (customer_id, clv_value, date, etc.)
  - [x] Match expected schema for Risk data (risk_id, risk_score, category, etc.)
  - [x] Generate minimum 50 rows per dataset
  - [x] Document synthetic data generation process in `docs/external-dependencies.md`
  - [x] NOTE: Synthetic data NOT needed - test data adequate (100 rows each)

## Dev Notes

### Project Context
This is **Story 0.2** in Epic 0 (Prerequisites & External Dependency Validation). This story depends on Story 0.1 confirming repository locations.

**Dependency:** Story 0.1 must identify test data directories before this story can validate their contents.

**Epic 4 Impact:** Epic 4, Story 4.1 requires augmented test data with tenant_id for multi-tenant filtering. This story prepares that data.

### Previous Story Insights
From Story 0.1:
- Repository locations documented in `docs/external-dependencies.md`
- Dash app structures inspected
- If repos not accessible, fallback plan documented

### Project Structure
[Source: architecture/11-unified-project-structure.md]

Expected test data locations (to verify):
```
sample-plotly-repos/
├── burn-performance-test-data/     # CLV test data
│   ├── *.csv or *.parquet or *.json
└── mixshift-test-data/             # Risk analysis test data
    ├── *.csv or *.parquet or *.json
```

Future data locations (Epic 4):
```
kyros-saas-poc/
├── data/
│   └── mock-data/                  # Augmented test data with tenant_id
│       ├── clv_data.csv
│       └── risk_data.csv
```

### Tech Stack for Data Handling
[Source: architecture/3-tech-stack.md]

**Data handling technologies:**
- **Python:** 3.11+
- **Pandas:** Latest (for DataFrame operations)
- **File formats:** CSV (primary), Parquet (if present), JSON (fallback)
- **In-Memory Data Store:** Pandas DataFrames loaded at startup in Epic 4

Data loading patterns:
```python
import pandas as pd

# CSV
df = pd.read_csv('data.csv')

# Parquet
df = pd.read_parquet('data.parquet')

# JSON
df = pd.read_json('data.json')

# Schema inspection
print(df.dtypes)
print(df.head())
print(df.shape)
```

### Multi-Tenant Data Model
[Source: architecture/4-data-models.md#47-mock-tenant-data]

**Tenant IDs from seed data:**
- Tenant 1 (Acme Corporation): `8e1b3d5b-7c9a-4e2f-b1d3-a5c7e9f12345`
- Tenant 2 (Beta Industries): `2450a2f8-3b7e-4eab-9b4a-1f73d9a0b1c4`

**Data distribution strategy:**
- Acme gets both CLV and Risk data
- Beta gets only Risk data (per Epic 0 specification)
- Split data: 60% Acme, 40% Beta for Risk data
- 100% Acme for CLV data (Beta has no CLV dashboard assignment)

**tenant_id augmentation:**
```python
# Add tenant_id column
df['tenant_id'] = 'UUID-HERE'

# Split by row index for multi-tenant
df_acme = df.iloc[:60].copy()
df_acme['tenant_id'] = '8e1b3d5b-7c9a-4e2f-b1d3-a5c7e9f12345'

df_beta = df.iloc[60:].copy()
df_beta['tenant_id'] = '2450a2f8-3b7e-4eab-9b4a-1f73d9a0b1c4'

# Combine
df_augmented = pd.concat([df_acme, df_beta])
```

### Testing
[Source: architecture/15-testing-strategy.md]

**No automated tests required** - this is a data validation task.

**Manual validation:**
- Load data files successfully into Pandas
- Verify row counts meet minimum threshold
- Confirm data types are compatible
- Visual inspection of schema documentation

**Definition of Done:**
- Test data locations verified and documented
- Data schemas documented in `docs/external-dependencies.md`
- tenant_id augmentation requirements identified
- Epic 4 data preparation steps documented
- Synthetic data created if needed

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-07 | 0.1 | Initial story creation | Bob (Scrum Master) |
| 2025-10-07 | 1.0 | Story completed - test data validated and documented | James (Dev Agent) |

## Dev Agent Record

### Agent Model Used
Claude 3.5 Sonnet (claude-sonnet-4-5-20250929)

### Debug Log References
None required - straightforward validation task

### Completion Notes List
- All 6 tasks completed successfully
- Test data directories found and accessible
- Data loading validation results:
  - ✅ burn.csv: 100 rows, 56 columns, CSV format
  - ✅ mix.csv: 100 rows, 6 columns, CSV format
  - Both files load successfully into Pandas DataFrames
- Data quality assessment:
  - burn.csv: Contains some null values (33 rows) - acceptable for time-series data
  - mix.csv: No null values
  - Both meet minimum threshold (100 >> 20 rows)
- tenant_id augmentation requirements documented:
  - Neither dataset has tenant_id column (expected)
  - Distribution plan: burn 100% Acme, mix 60% Acme / 40% Beta
  - Full augmentation script provided for Epic 4
- Epic 4 data preparation steps comprehensively documented:
  - 5-step process with code snippets
  - Data API integration pattern provided
  - Validation checks documented
- Synthetic data generation NOT needed - existing test data adequate
- Created validation script at `test_data_loading.py` (can be reused)
- All acceptance criteria met
- Documentation added to `docs/external-dependencies.md`:
  - Complete data schemas (column names, types, row counts)
  - Tenant augmentation requirements and strategy
  - Epic 4 preparation steps with code examples

### File List
**Created:**
- `test_data_loading.py` - Python script for validating test data loading into Pandas (reusable for future validation)

**Modified:**
- `docs/external-dependencies.md` - Added comprehensive test data documentation including schemas, tenant augmentation requirements, and Epic 4 preparation steps

## QA Results
_(To be filled by QA Agent)_
